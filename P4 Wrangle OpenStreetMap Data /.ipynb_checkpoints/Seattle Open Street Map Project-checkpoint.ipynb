{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling with MongoDB\n",
    "### Thuy Quach\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map area: Seattle, WA, United States\n",
    "\n",
    "http://www.openstreetmap.org/relation/237385\n",
    "\n",
    "https://mapzen.com/data/metro-extracts/#seattle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract:\n",
    "\n",
    "Open Street Map is a collaborative project to create a free editable map of the world. It is a great source of data. However as any open source data, it contains many inconsistent or incorrect information such as street types, postcodes and city names. The tasks of the project was to audit, fixing and processing the dataset of Seattle. MongoDB queries were used to obtain the overview of the data is and also other investigations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parse the download oms file and take a sample part of the map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of original seattle.osm was 1.54GB. To make it run with my PC, I parsed the file and took a sample of 1/20th of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parse file and make a sample file\n",
    "import xml.etree.cElementTree as ET\n",
    "OMS_FILE = \"seattle_washington.osm\"\n",
    "SAMPLE_FILE = \"seattle_sample.osm\"\n",
    "def get_element(oms_file, tags = ('node', 'way', 'relation')):\n",
    "    context = ET.iterparse(oms_file, events = ('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n')\n",
    "    \n",
    "    #write every 100th top level element\n",
    "    for i, element in enumerate(get_element(OMS_FILE)):\n",
    "        if i%20 == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import IPython.core.display as di\n",
    "\n",
    "# This line will hide code by default when the notebook is exported as HTML\n",
    "di.display_html('<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>', raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Problems encountered in the map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check problematic characters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before process the data and add it into MongoDB, we should check the \"k\"\n",
    "value for each \"tag\" and see if they can be valid keys in MongoDB, as well as\n",
    "see if there are any other potential problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\"\"\"\n",
    "  \"lower\", for tags that contain only lowercase letters and are valid,\n",
    "  \"lower_colon\", for otherwise valid tags with a colon in their names,\n",
    "  \"problemchars\", for tags with problematic characters, and\n",
    "  \"other\", for other tags that do not fall into the other three categories.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        # YOUR CODE HERE\n",
    "        k = element.get(\"k\")\n",
    "        #print k\n",
    "        if re.search(lower, k):\n",
    "            keys['lower'] += 1\n",
    "        elif re.search(lower_colon, k):\n",
    "            keys['lower_colon'] += 1\n",
    "        elif re.search(problemchars, k):\n",
    "            keys['problemchars'] += 1\n",
    "        else:\n",
    "            keys['other'] += 1\n",
    "        pass\n",
    "        \n",
    "    return keys\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_map_key_type(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lower': 102074, 'lower_colon': 114390, 'other': 3692, 'problemchars': 0}"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_map_key_type('seattle_sample.osm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are no tags with problematic characters.\n",
    "- There are 102074 tags that contain only lowercase letters and are valid.\n",
    "- There are 114390 for otherwise valid tags with a colon in their names.\n",
    "- There are  3692 tags that do not fall into the other three categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems with street types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to  audit the OSMFILE to see the street types are appropriate ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "audit the OSMFILE reflect the changes needed to fix the unexpected street types to the appropriate ones in the expected list.\n",
    "\"\"\"\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    #print street_types\n",
    "    return street_types\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the list of street types:\n",
      "['Glen', 'Fir', 'Northeast', 'Ridge', 'West', 'St.', 'Heights', 'Rd', 'Speedway', 'street', 'Blackburn', 'Northwest', 'Way', '1st', 'Gate', 'Circle', 'East', 'avenue', 'Meadows', 'Highway', 'Southwest', '104', 'Cleveland', 'North', 'west', 'Rise', 'Reach', 'NE', 'Meridian', 'Southeast', '18th', 'Loop', 'Hwy', '9', 'Snoqualmie', '25th', 'Section', 'NW', 'Laventure', 'E', 'Center', 'Sandalwood', '36th', 'Plaza', 'Alley', 'St', '99', 'Division', 'S', 'Green', 'W', 'Close', '20', 'N', 'Estates', 'Gardens', 'South', 'Point', 'S.E.', 'Terrace', 'SW', 'Esplanade', 'r', 'Crescent', 'Waugh', 'Broadway', 'southwest', 'SE']\n"
     ]
    }
   ],
   "source": [
    "print \"Here is the list of street types:\" + \"\\n\" + str(audit('seattle_sample.osm').keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that the street types were over-abbreviated such as \"S.E\", \"N\", \"S\", etc. We need to fix the unexpected street names to the appropriate ones by using mapping dictionary and then updated the street name. We will later use it to update the street names in json data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Update street names\n",
    "mapping = { \"SE\": \"South East\",\n",
    "            \"St.\": \"Street\", \n",
    "            \"Rd\": \"Road\",\n",
    "            \"SouthEast\" : \"South East\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Northeast\": \"North East\",\n",
    "            \"Northwest\" : \"North West\",\n",
    "            \"Southwest\" : \"South West\",\n",
    "            \"N\" : \"North\",\n",
    "            \"E\" : \"East\",\n",
    "            \"W\" : \"West\",\n",
    "            \"S\" : \"South\",\n",
    "           \"S.E\": \"South East\",\n",
    "           \"SW\" : \"South West\"\n",
    "            }\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    for word in mapping.keys():\n",
    "        name = re.sub(r'\\b' + word + r'\\b\\.?',  mapping[word], name)\n",
    "    return name\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Problems with postcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postcode is also important data. All the postcodes in Seattle start with 981. Let's check to see all the postcode are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "audit the OSMFILE reflect the changes needed to fix the unexpected postcode types to the appropriate ones in the expected list.\n",
    "\"\"\"\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "# check if the first three characters start with 981\n",
    "postcode_re = re.compile(r'^981')\n",
    "\n",
    "# initiate the not match postcode\n",
    "postcode_diff = []\n",
    "\n",
    "def audit_postcode(postcode_re, postcode_num):\n",
    "    if not postcode_re.match(postcode_num):\n",
    "        if postcode_num not in postcode_diff:\n",
    "            postcode_diff.append(postcode_num)\n",
    "\n",
    "\n",
    "def is_postcode_num(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "\n",
    "def audit_postcode_name(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_postcode_num(tag):   \n",
    "                    audit_postcode(postcode_re, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return postcode_diff\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audit_postcode_name(\"seattle_sample.osm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 128 unmatched postcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of ummacthed postcode: \n",
      "['98070', 'V8X 4V1', '98004', '98005', '98273', '98503', '98011', '98057', '98002', '98052']\n"
     ]
    }
   ],
   "source": [
    "print \"Some of ummacthed postcode: \" + \"\\n\" + str(audit_postcode_name(\"seattle_example.osm\")[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quick checking some of the postcodes are in other cities next to Seattle such as Kirkland and Bellevue. It would be interesting to find out later how many of the data are from those cities using MongoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem with city name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the postcode data, there was some data in different cities than Seattle in the dataset. I will later check with MongoDB queries to see more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare the data set for MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the oms to json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The oms file was parsed. The output data was shaped by its elements and then wrote on json file. The json file then was imported to MongoDB for analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "\n",
    "def shape_element(element):\n",
    "    node = {}\n",
    "    created ={}\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        \n",
    "        for e in element.attrib.keys():\n",
    "            if e in CREATED:\n",
    "                created[e] = element.get(e)\n",
    "                node[\"created\"] = created\n",
    "        \n",
    "            \n",
    "        node[\"id\"] = element.get('id')\n",
    "        node[\"type\"] = element.tag\n",
    "        node[\"visible\"] = element.get('visible')\n",
    "        pos = [element.get('lat'), element.get('lon')]\n",
    "        for i in pos:\n",
    "            if type(i) == str:\n",
    "                node['pos'] = [float(i) for i in pos]              \n",
    "                          \n",
    "        address = {}\n",
    "        node_refs = []\n",
    "            \n",
    "        for tag in element:\n",
    "            \n",
    "            if tag.tag == \"tag\":\n",
    "                k = tag.get('k')\n",
    "                v = tag.get('v')\n",
    "                if re.search('problemchars', k):\n",
    "                    pass\n",
    "                elif re.search(r'\\w+:\\w+:\\w+', k):\n",
    "                    pass\n",
    "                elif k.startswith('addr:'): \n",
    "                    address[k[5:]] = v\n",
    "                    node['address'] = address\n",
    "                else:\n",
    "                    node[k] = v\n",
    "            \n",
    "\n",
    "            if tag.tag == 'nd':\n",
    "                ref = tag.get('ref')\n",
    "                #print type(ref)\n",
    "                node_refs.append(tag.attrib['ref'])\n",
    "                node['node_refs'] = node_refs\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def process_map(file_in, pretty = False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = process_map('seattle_sample.osm', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auditting data with MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the postcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MongoDB queries were used to analyze top postcodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import mongoDB \n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['test']\n",
    "\n",
    "# return the aggregation\n",
    "# the collection name seattle1\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.seattle2.aggregate(pipeline)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 postcode:\n",
      "[{u'count': 1147, u'_id': u'98034'}, {u'count': 968, u'_id': u'98033'}, {u'count': 905, u'_id': u'98115'}, {u'count': 838, u'_id': u'98103'}, {u'count': 716, u'_id': u'98118'}, {u'count': 692, u'_id': u'98117'}, {u'count': 610, u'_id': u'98125'}, {u'count': 492, u'_id': u'98105'}, {u'count': 470, u'_id': u'98108'}, {u'count': 463, u'_id': u'98144'}]\n"
     ]
    }
   ],
   "source": [
    "postcode_queries = [{\"$match\": {\"address.postcode\": {\"$exists\": 1}}},\n",
    "                   {\"$group\": {\"_id\": \"$address.postcode\",\n",
    "                              \"count\": {\"$sum\":1}}},\n",
    "                   {\"$sort\": {\"count\": -1}}]\n",
    "result = aggregate(db, postcode_queries)\n",
    "# top 10 results\n",
    "\n",
    "print \"Top 10 postcode:\" + \"\\n\" + str(result[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of top 10 postcodes, two are in Kirkland (start with 980) and the rest are in Seattle (start with 981). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data with postcode is 13219 out of 378925 data.\n"
     ]
    }
   ],
   "source": [
    "# all data with postcode\n",
    "postcode_all = [{\"$match\": {\"address.postcode\": {\"$exists\": 1}}}]\n",
    "result = aggregate(db, postcode_all)\n",
    "print \"Total data with postcode is \" + str(len(result)) + \" out of \" + str(db.seattle1.find().count()) + \" data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data with Seattle postcode is 10162\n"
     ]
    }
   ],
   "source": [
    "# data with Seattle postcode\n",
    "postcode_Seattle = [{\"$match\": {\"address.postcode\": {\"$exists\": 1, '$regex': '^981'}}}]\n",
    "result = aggregate(db, postcode_Seattle)\n",
    "print \"Total data with Seattle postcode is \" + str(len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many data don't have postcode. Among them, most are Seattle zipcode. Therefore we can remove the data that has postcode unmatched '981'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the city name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MongoDB queries were used to analyze all city names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 cities: [{u'count': 10143, u'_id': u'Seattle'}, {u'count': 2113, u'_id': u'Kirkland'}, {u'count': 606, u'_id': u'Saanich'}, {u'count': 559, u'_id': u'Mount Vernon'}, {u'count': 138, u'_id': u'Langford'}, {u'count': 115, u'_id': u'Oak Bay'}, {u'count': 92, u'_id': u'Colwood'}, {u'count': 79, u'_id': u'Esquimalt'}, {u'count': 78, u'_id': u'Sooke'}, {u'count': 49, u'_id': u'Metchosin'}]\n",
      "\n",
      "Number of cities: 82\n"
     ]
    }
   ],
   "source": [
    "# city names\n",
    "city_queries = [{\"$match\": {\"address.city\": {\"$exists\": 1}}},\n",
    "                   {\"$group\": {\"_id\": \"$address.city\",\n",
    "                              \"count\": {\"$sum\":1}}},\n",
    "                   {\"$sort\": {\"count\": -1}}]\n",
    "result_city = aggregate(db, city_queries)\n",
    "print \"Top 10 cities: \" + str(result_city[:10]) + \"\\n\"\n",
    "\n",
    "print \"Number of cities: \" + str(len(result_city))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some cites are not from Seattle such as Kirkland, Saanich, Langford, Oak Bay etc. Kirkland is a neighbor city of Seattle. Saanich and Oak Bay are two cities in Victoria, Canada. They are both nearby cities to Seattle. There are total of 81 cities are not Seattle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, after running the analysis, we need to clean the street types, postcode and city names. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update street name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data after updating street types: 378925\n"
     ]
    }
   ],
   "source": [
    "# Update street name\n",
    "def update_streetname(data):\n",
    "    for entry in data:\n",
    "        if 'address' in entry.keys() and 'street' in entry['address']:\n",
    "            name = entry['address']['street']\n",
    "            entry['address']['street'] = update_name(name, mapping)\n",
    "    return data\n",
    "\n",
    "clean_name_data = update_streetname(data)\n",
    "print \"Total data after updating street types: \" + str(len(clean_name_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop data with postcode unmatch '981'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data after updating street types and postcode: 375868\n"
     ]
    }
   ],
   "source": [
    "# drop data with postcode unmatch 981\n",
    "\n",
    "def remove_postcode(data):\n",
    "    returned_data = []\n",
    "    for entry in data:\n",
    "        if 'address' in entry.keys() and 'postcode' in entry['address'] and not entry['address']['postcode'].startswith('981'):\n",
    "            continue\n",
    "        else:\n",
    "            returned_data.append(entry)    \n",
    "    return returned_data\n",
    "            \n",
    "clean_postcode_data = remove_postcode(clean_name_data)\n",
    "print \"Total data after updating street types and postcode: \" + str(len(clean_postcode_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Drop city name unmatched 'Seattle' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data after updating street types, postcode and city names : 374540\n"
     ]
    }
   ],
   "source": [
    "# drop city with name different with Seattle\n",
    "\n",
    "def remove_cityname(data):\n",
    "    returned_data = []\n",
    "    for entry in data:\n",
    "        if 'address' in entry.keys() and 'city' in entry['address'] and entry['address']['city'].lower() != 'seattle':\n",
    "            continue\n",
    "        else:\n",
    "            returned_data.append(entry)    \n",
    "    return returned_data\n",
    "            \n",
    "updated_data = remove_cityname(clean_postcode_data)\n",
    "print \"Total data after updating street types, postcode and city names : \" + str(len(updated_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write clean data to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write data to json\n",
    "with codecs.open('clean_seattle3.json', 'w') as f:\n",
    "    f.write(json.dumps(updated_data)+ \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data into Mongo DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import pymongo\n",
    "\n",
    "# Get database connection db first\n",
    "\n",
    "with open('clean_seattle3.json', 'rb') as json_file:\n",
    "    for line in json_file:\n",
    "        db.clean_seattle3.insert(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4. Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains basic statistics about the dataset. MongoDB queries are used to get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "def aggregate_clean(db, pipeline):\n",
    "    return [doc for doc in db.clean_seattle3.aggregate(pipeline)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name : clean_seattle3.json \n",
      "File size: 91MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print \"File name : clean_seattle3.json \" + \"\\n\" + \"File size: \" + str(os.path.getsize('clean_seattle3.json')/1000000) + \"MB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 374540\n"
     ]
    }
   ],
   "source": [
    "# number of documents\n",
    "print \"Number of documents: \" + str(db.clean_seattle3.find().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 343065\n"
     ]
    }
   ],
   "source": [
    "# number of node\n",
    "print \"Number of nodes: \" + str(db.clean_seattle3.find({\"type\": \"node\"}).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ways: 94413\n"
     ]
    }
   ],
   "source": [
    "# number of ways\n",
    "print \"Number of ways: \" + str(db.clean_seattle2.find({\"type\": \"way\"}).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 1447\n"
     ]
    }
   ],
   "source": [
    "unique_users =    [\n",
    "                   {\"$group\": {\"_id\": \"$created.user\",\n",
    "                              \"count\": {\"$sum\":1}}},\n",
    "                   {\"$sort\": {\"count\": -1}}]\n",
    "\n",
    "\n",
    "print \"Number of unique users: \" + str(len(aggregate_clean(db, unique_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1st user: {u'count': 61392, u'_id': u'Glassman'}\n"
     ]
    }
   ],
   "source": [
    "# top contributing users\n",
    "\n",
    "print (\"Top 1st user: \" + str(aggregate_clean(db, unique_users)[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10th user: \n",
      "[{u'count': 61392, u'_id': u'Glassman'}, {u'count': 37383, u'_id': u'SeattleImport'}, {u'count': 33100, u'_id': u'tylerritchie'}, {u'count': 31165, u'_id': u'woodpeck_fixbot'}, {u'count': 16124, u'_id': u'alester'}, {u'count': 11278, u'_id': u'STBrenden'}, {u'count': 10735, u'_id': u'Glassman_Import'}, {u'count': 8995, u'_id': u'Brad Meteor'}, {u'count': 8441, u'_id': u'Amoebabadass'}, {u'count': 6041, u'_id': u'zephyr'}]\n"
     ]
    }
   ],
   "source": [
    "# top 10 contributing users\n",
    "\n",
    "print (\"Top 10th user: \" + \"\\n\" + str(aggregate_clean(db, unique_users)[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5. Additional analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving OMS data quality suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Address with over-abbreviated and inconsistent street types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The street types data could be improved by let the contributing users enter an auto-corrected street types. Each countries and/or cities may have different street types. If possible, the street type data of interested area could be collected via cross-reference sources such as local transit goverment, postmail company or Google Maps. In case there is no standard existing street types, the local open street map community could make one data processor similar to my auditing and updating street types functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unmatched postcode and city name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to street types data, postcode data could be much improved by having an standard postcode. If the new data's postcode does not in the appropriate list, it should be warned. Same with city name, it should be same with city of interest. A data processor similar to my auditing and fixing postcode and city name could work well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encourage user parcipation and collaboration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the user data, there are 1447 users in which top 10 user contribution is 60%. In top 10 user there is only one name appears with the word 'bot'. It looks like it could have a great chance of collaboration to make and edit beter map. Certain gamification elements, such as badges or leaderboard would strive to leverage user's desires to create better data and community collaboration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional data exploration using MongoDB queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top 10 amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'_id': u'parking', u'count': 383},\n",
       " {u'_id': u'bicycle_parking', u'count': 149},\n",
       " {u'_id': u'school', u'count': 123},\n",
       " {u'_id': u'restaurant', u'count': 117},\n",
       " {u'_id': u'bench', u'count': 94},\n",
       " {u'_id': u'place_of_worship', u'count': 69},\n",
       " {u'_id': u'cafe', u'count': 53},\n",
       " {u'_id': u'waste_basket', u'count': 48},\n",
       " {u'_id': u'fuel', u'count': 43},\n",
       " {u'_id': u'fast_food', u'count': 43}]"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10_amenities = [{\"$match\": {\"amenity\": {\"$exists\": 1}}},\n",
    "                   {\"$group\": {\"_id\": \"$amenity\",\n",
    "                              \"count\": {\"$sum\":1}}},\n",
    "                   {\"$sort\": {\"count\": -1}},\n",
    "                   {\"$limit\": 10}]\n",
    "aggregate_clean(db, top10_amenities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that 'parking' is the top amenity in Seattle. It is not a surprise given Seattle is a home city of many companies. 'Bicycle_parking', 'bench' and 'waste_basket' are also in the top 10 amenity. There was also a lot of cafe. It could be Seattle is home of Starbucks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top 10 cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'_id': None, u'count': 49},\n",
       " {u'_id': u'pizza', u'count': 9},\n",
       " {u'_id': u'american', u'count': 7},\n",
       " {u'_id': u'mexican', u'count': 6},\n",
       " {u'_id': u'chinese', u'count': 5},\n",
       " {u'_id': u'thai', u'count': 4},\n",
       " {u'_id': u'sandwich', u'count': 4},\n",
       " {u'_id': u'italian', u'count': 4},\n",
       " {u'_id': u'japanese', u'count': 3},\n",
       " {u'_id': u'regional', u'count': 2}]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10_cuisine = [{\"$match\": {\"amenity\": {\"$exists\": 1}, \"amenity\": \"restaurant\"}},\n",
    "                   {\"$group\": {\"_id\": \"$cuisine\",\n",
    "                              \"count\": {\"$sum\":1}}},\n",
    "                   {\"$sort\": {\"count\": -1}},\n",
    "                   {\"$limit\": 10}]\n",
    "aggregate_clean(db, top10_cuisine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The top 1st restaurant had 'None' name. It should be included since it accounts to about half of the total cuisines. Seattle seems to have a good variety of cuisines from 'pizza' to 'mexcian', 'italian' and other asian ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seattle open street map dataset is a good data source for data wrangling purpose. It contains inconsistant street types; postcodes that are not in Seattle; and cities are in nearby cities of Seattle or even in Canada. They are all cleaned. Over view of the cleaned data was reported. Some interesting data such as top amenities and cuisine were analyzed. \n",
    "\n",
    "To improve the OMS data quality, I suggested to have a cross-reference with other data sources such as local goverment data, post mail company or Google Maps to ensure the correctness and consistance of the OMS data. If those cross-reference data are not available, the OMS community could collaborate and come up with their own standard data and clean it as my way in this project. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
